{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Instalando as dependências do SINAPSES\n",
        "#!pip install -r /content/requirements.txt"
      ],
      "metadata": {
        "id": "oTB-syzwIBkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "from typing import Callable\n",
        "from sinapses_cliente.modelo.binario import Binario\n",
        "from sinapses_cliente.modelo.modelo_versao_recurso import ModeloVersaoRecurso\n",
        "from sinapses_cliente.modelo.resposta.resposta_classificacao import Resposta\n",
        "from sinapses_cliente.modelo.servico_contexto import ServicoContexto\n",
        "from sinapses_cliente.modelo.tipo_conteudo import TipoConteudo\n",
        "from sinapses_cliente.modelo.treinamento_resultado import TreinamentoResultado\n",
        "from sinapses_cliente.sinapses_pipeline import SinapsesPipeline\n",
        "from sinapses_cliente.sinapses_sessao import SinapsesSessao\n",
        "from sinapses_cliente.modelo.requisicao.mensagem import Mensagem\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib"
      ],
      "metadata": {
        "id": "PnSTvp_6H9l2"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def funcao_treinamento(sessao: SinapsesSessao) -> TreinamentoResultado:\n",
        "    # === 1. Carregar dados ===\n",
        "    with open(\"dados_salvos.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # === 2. Criar DataFrame com textos dos movimentos ===\n",
        "    records = []\n",
        "    for item in data[\"hits\"][\"hits\"]:\n",
        "        proc = item[\"_source\"]\n",
        "        numero = proc.get(\"numeroProcesso\")\n",
        "        classe = proc.get(\"classe\", {}).get(\"nome\", \"\")\n",
        "        assunto = \"; \".join(a[\"nome\"] for a in proc.get(\"assuntos\", []) if \"nome\" in a)\n",
        "        movimentos = \" > \".join(m[\"nome\"] for m in proc.get(\"movimentos\", []) if \"nome\" in m)\n",
        "\n",
        "        records.append({\n",
        "            \"numeroProcesso\": numero,\n",
        "            \"classe\": classe,\n",
        "            \"assunto\": assunto,\n",
        "            \"movimentos_texto\": movimentos\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    # === 3. Vetorizar os movimentos ===\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_text = vectorizer.fit_transform(df[\"movimentos_texto\"])\n",
        "\n",
        "    # === 4. Gerar rótulos com Isolation Forest ===\n",
        "    modelo_iso = IsolationForest(contamination=0.1, random_state=42)\n",
        "    y_iso = modelo_iso.fit_predict(X_text)\n",
        "\n",
        "    # Normalizar rótulo para 0 (normal) e 1 (anomalia)\n",
        "    df[\"anomalia\"] = np.where(y_iso == -1, 1, 0)\n",
        "\n",
        "    # === 5. Preparar dados para rede neural ===\n",
        "    X = X_text.toarray().astype(np.float32)\n",
        "    y = df[\"anomalia\"].values.astype(np.float32)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # === 6. Definir rede neural ===\n",
        "    modelo = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(X.shape[1],)),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')  # saída binária\n",
        "    ])\n",
        "\n",
        "    modelo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # === 7. Treinar ===\n",
        "    early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "    history = modelo.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stop])\n",
        "\n",
        "    # === 8. Avaliação ===\n",
        "    loss, accuracy = modelo.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    y_pred = (modelo.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    relatorio = classification_report(y_test, y_pred, target_names=[\"normal\", \"anomalia\"])\n",
        "\n",
        "    with open(\"relatorio_rede_neural_tfidf.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(relatorio)\n",
        "\n",
        "    # === 9. Salvar modelo e vetor TF-IDF ===\n",
        "    modelo.save(\"modelo_rede_neural_tfidf.h5\")\n",
        "\n",
        "    joblib.dump(vectorizer, \"vetorizador_tfidf.pkl\")\n",
        "\n",
        "    # === 10. Gráficos ===\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='loss')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss')\n",
        "    plt.title('Perda')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='acc')\n",
        "    plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "    plt.title('Acurácia')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"treinamento_rede_neural_tfidf.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # === Salvar arquitetura do modelo em JSON ===\n",
        "    modelo_json = modelo.to_json()\n",
        "    with open(\"modelo.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json_file.write(modelo_json)\n",
        "\n",
        "    # print(\"Rede neural treinada com vetores TF-IDF e rótulos do Isolation Forest.\")\n",
        "\n",
        "    # Retornar os resultados\n",
        "    return TreinamentoResultado(accuracy, [\n",
        "        ModeloVersaoRecurso('treinamento_rede_neural_tfidf.png',\n",
        "                            Binario.from_arquivo('treinamento_rede_neural_tfidf.png'),\n",
        "                            TipoConteudo.IMAGE_PNG,\n",
        "                            'GRAFICO_TREINAMENTO'),\n",
        "        ModeloVersaoRecurso('dados_salvos.json', Binario.from_arquivo('dados_salvos.json'),\n",
        "                            TipoConteudo.APPLICATION_JSON,\n",
        "                            'DADOS_TREINAMENTO'),\n",
        "        ModeloVersaoRecurso('modelo.json', Binario(bytes(modelo_json, 'UTF-8')), TipoConteudo.APPLICATION_JSON,\n",
        "                            'MODELO_TREINADO'),\n",
        "        ModeloVersaoRecurso('modelo.pesos.h5', Binario.from_arquivo('modelo_rede_neural_tfidf.h5'),\n",
        "                            TipoConteudo.APPLICATION_OCTET_STREAM, 'MODELO_TREINADO')\n",
        "    ])"
      ],
      "metadata": {
        "id": "ktCh-L5KI6ld"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def funcao_inicializacao_servico(sessao: SinapsesSessao):\n",
        "    modelo_versao = sessao.get_modelo_versao()\n",
        "    modelo = model_from_json(modelo_versao.get_recurso_por_nome('modelo.json').conteudo.bytes)\n",
        "    arquivo_modelo_weights = str(sessao.get_diretorio_trabalho()) + '/modelo_weights.h5'\n",
        "    modelo_versao.get_recurso_por_nome('modelo.pesos.h5').conteudo.salvar_em(arquivo_modelo_weights)\n",
        "    modelo.load_weights(arquivo_modelo_weights)\n",
        "\n",
        "    logger = logging.getLogger('Sinapses')\n",
        "    logger.setLevel(logging.ERROR)\n",
        "    formatter = logging.Formatter(fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "    stream_handler = logging.StreamHandler(sys.stdout)\n",
        "    stream_handler.setFormatter(formatter)\n",
        "    logger.addHandler(stream_handler)\n",
        "\n",
        "    return ServicoContexto(\n",
        "        modelo=modelo,\n",
        "        logger=logger\n",
        "    )"
      ],
      "metadata": {
        "id": "y4kFV1xeI-r1"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def funcao_servico(sessao: SinapsesSessao, ctx: ServicoContexto, requisicao: dict) -> Resposta:\n",
        "    resultado = \"\"\n",
        "\n",
        "    # Carregar o vetorizador apenas uma vez\n",
        "    vectorizer = joblib.load(\"vetorizador_tfidf.pkl\")\n",
        "\n",
        "    # Extrair os movimentos do dicionário da requisição\n",
        "    movimentos_raw = requisicao.get(\"movimentos\", [])\n",
        "\n",
        "    # Reorganizar para obter os nomes dos movimentos\n",
        "    movimentos = []\n",
        "    for mov in movimentos_raw:\n",
        "        nome_info = mov.get(\"nome\", {})\n",
        "        if nome_info.get(\"tipo\") == \"TEXTO_PURO\" and \"conteudo\" in nome_info:\n",
        "            movimentos.append(nome_info[\"conteudo\"])\n",
        "\n",
        "    if len(movimentos) < 4:\n",
        "        resultado = \"Não foi possível avaliar\"\n",
        "    else:\n",
        "        # Concatenar nomes com separador\n",
        "        texto_movimentos = \" > \".join(movimentos)\n",
        "\n",
        "        # Vetorizar o texto com TF-IDF\n",
        "        X_novo = vectorizer.transform([texto_movimentos]).toarray()\n",
        "\n",
        "        # Fazer a predição com o modelo\n",
        "        pred = ctx.modelo.predict(X_novo)[0][0]\n",
        "\n",
        "        # Interpretar o resultado\n",
        "        if pred >= 0.5:\n",
        "            resultado = \"Possui anomalias na movimentação\"\n",
        "        else:\n",
        "            resultado = \"Não possui anomalias na movimentação\"\n",
        "\n",
        "    # print(resultado)\n",
        "\n",
        "    return Resposta(**{\"resultado\": resultado})"
      ],
      "metadata": {
        "id": "ztzP7fc8UBOC"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "iJhT_Jz7Hkaj"
      },
      "outputs": [],
      "source": [
        "def funcao_teste_servico(_s: SinapsesSessao, _c: ServicoContexto, servico: Callable[[dict], dict]) -> None:\n",
        "    r1 = servico({\n",
        "        \"classe\": Mensagem.texto(\"Procedimento do Juizado Especial Cível\").to_dict(),\n",
        "        \"assunto\": Mensagem.texto(\"Pensão por Morte (Art. 74/9)\").to_dict(),\n",
        "        \"numeroProcesso\": Mensagem.texto(\"50013586020254025103\").to_dict(),\n",
        "        \"movimentos\": [\n",
        "            {\n",
        "                \"codigo\": Mensagem.texto(\"26\").to_dict(),\n",
        "                \"nome\": Mensagem.texto(\"Distribuição\").to_dict(),\n",
        "                \"dataHora\": Mensagem.texto(\"2025-02-26T13:02:29.000Z\").to_dict()\n",
        "            },\n",
        "            {\n",
        "                \"codigo\": Mensagem.texto(\"12164\").to_dict(),\n",
        "                \"nome\": Mensagem.texto(\"Outras Decisões\").to_dict(),\n",
        "                \"dataHora\": Mensagem.texto(\"2025-03-10T18:09:31.000Z\").to_dict()\n",
        "            },\n",
        "            {\n",
        "                \"codigo\": Mensagem.texto(\"11010\").to_dict(),\n",
        "                \"nome\": Mensagem.texto(\"Mero expediente\").to_dict(),\n",
        "                \"dataHora\": Mensagem.texto(\"2025-03-21T17:25:24.000Z\").to_dict()\n",
        "            },\n",
        "            {\n",
        "                \"codigo\": Mensagem.texto(\"11010\").to_dict(),\n",
        "                \"nome\": Mensagem.texto(\"Mero expediente\").to_dict(),\n",
        "                \"dataHora\": Mensagem.texto(\"2025-03-25T21:20:27.000Z\").to_dict()\n",
        "            },\n",
        "            {\n",
        "                \"codigo\": Mensagem.texto(\"898\").to_dict(),\n",
        "                \"nome\": Mensagem.texto(\"Por decisão judicial\").to_dict(),\n",
        "                \"dataHora\": Mensagem.texto(\"2025-03-26T23:19:27.000Z\").to_dict()\n",
        "            },\n",
        "            {\n",
        "                \"codigo\": Mensagem.texto(\"12067\").to_dict(),\n",
        "                \"nome\": Mensagem.texto(\"Levantamento da Suspensão ou Dessobrestamento\").to_dict(),\n",
        "                \"dataHora\": Mensagem.texto(\"2025-04-30T18:33:56.000Z\").to_dict()\n",
        "            }\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    r2 = servico(\n",
        "        {\n",
        "            \"classe\": Mensagem.texto(\"Cumprimento de Sentença contra a Fazenda Pública\").to_dict(),\n",
        "            \"assunto\": Mensagem.texto(\"Rural (Art. 48/51)\").to_dict(),\n",
        "            \"numeroProcesso\": Mensagem.texto(\"50001245220254025003\").to_dict(),\n",
        "            \"movimentos\": [\n",
        "                {\n",
        "                    \"codigo\": Mensagem.texto(\"26\").to_dict(),\n",
        "                    \"nome\": Mensagem.texto(\"Distribuição\").to_dict(),\n",
        "                    \"dataHora\": Mensagem.texto(\"2025-01-15T23:18:51.000Z\").to_dict()\n",
        "                },\n",
        "                {\n",
        "                    \"codigo\": Mensagem.texto(\"85\").to_dict(),\n",
        "                    \"nome\": Mensagem.texto(\"Petição\").to_dict(),\n",
        "                    \"dataHora\": Mensagem.texto(\"2025-03-28T08:13:40.000Z\").to_dict()\n",
        "                },\n",
        "                {\n",
        "                    \"codigo\": Mensagem.texto(\"85\").to_dict(),\n",
        "                    \"nome\": Mensagem.texto(\"Petição\").to_dict(),\n",
        "                    \"dataHora\": Mensagem.texto(\"2025-03-30T17:02:46.000Z\").to_dict()\n",
        "                },\n",
        "                {\n",
        "                    \"codigo\": Mensagem.texto(\"85\").to_dict(),\n",
        "                    \"nome\": Mensagem.texto(\"Petição\").to_dict(),\n",
        "                    \"dataHora\": Mensagem.texto(\"2025-03-31T14:44:00.000Z\").to_dict()\n",
        "                },\n",
        "                {\n",
        "                    \"codigo\": Mensagem.texto(\"10966\").to_dict(),\n",
        "                    \"nome\": Mensagem.texto(\"Mudança de Classe Processual\").to_dict(),\n",
        "                    \"dataHora\": Mensagem.texto(\"2025-03-31T10:26:13.000Z\").to_dict()\n",
        "                },\n",
        "                {\n",
        "                    \"codigo\": Mensagem.texto(\"11010\").to_dict(),\n",
        "                    \"nome\": Mensagem.texto(\"Mero expediente\").to_dict(),\n",
        "                    \"dataHora\": Mensagem.texto(\"2025-01-23T16:08:02.000Z\").to_dict()\n",
        "                },\n",
        "                {\n",
        "                    \"codigo\": Mensagem.texto(\"466\").to_dict(),\n",
        "                    \"nome\": Mensagem.texto(\"Homologação de Transação\").to_dict(),\n",
        "                    \"dataHora\": Mensagem.texto(\"2025-01-29T16:15:25.000Z\").to_dict()\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    r3 = servico(\n",
        "        {\n",
        "            \"classe\": Mensagem.texto(\"Procedimento do Juizado Especial Cível\").to_dict(),\n",
        "            \"assunto\": Mensagem.texto(\"Pensão por Morte (Art. 74/9)\").to_dict(),\n",
        "            \"numeroProcesso\": Mensagem.texto(\"50013906520254025103\").to_dict(),\n",
        "            \"movimentos\": [\n",
        "                {\n",
        "                    \"codigo\": Mensagem.texto(\"26\").to_dict(),\n",
        "                    \"nome\": Mensagem.texto(\"Distribuição\").to_dict(),\n",
        "                    \"dataHora\": Mensagem.texto(\"2025-02-27T10:38:49.000Z\").to_dict()\n",
        "                },\n",
        "                {\n",
        "                    \"codigo\": Mensagem.texto(\"12164\").to_dict(),\n",
        "                    \"nome\": Mensagem.texto(\"Outras Decisões\").to_dict(),\n",
        "                    \"dataHora\": Mensagem.texto(\"2025-03-12T17:37:48.000Z\").to_dict()\n",
        "                },\n",
        "                {\n",
        "                    \"codigo\": Mensagem.texto(\"11010\").to_dict(),\n",
        "                    \"nome\": Mensagem.texto(\"Mero expediente\").to_dict(),\n",
        "                    \"dataHora\": Mensagem.texto(\"2025-04-07T13:19:04.000Z\").to_dict()\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    assert r1 is not None\n",
        "    assert r2 is not None\n",
        "    assert r3 is not None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import __main__ as main\n",
        "#main.__file__ = \"ia_revisor_inteligente.ipynb\""
      ],
      "metadata": {
        "id": "R0YnFlJFPe8c"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar o pipeline\n",
        "pipeline = SinapsesPipeline.instancia()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMDNZ9beJQCt",
        "outputId": "db56f157-bd17-41f0-96fa-f3eabd1707f0"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O arquivo de configuração foi encontrado em: /content/sinapses.yml\n",
            "Validando as configurações do arquivo encontrado...\n",
            " - O endereço do sinapses é válido!\n",
            " - A autenticação é válida!\n",
            " - O id da versão do modelo é válida!\n",
            "Arquivo de configuração válido!\n",
            "ERRO: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir as funções\n",
        "pipeline.definir_treinamento(funcao_treinamento)\n",
        "pipeline.definir_inicializacao_servico(funcao_inicializacao_servico)\n",
        "pipeline.definir_servico(funcao_servico)\n",
        "pipeline.definir_teste_servico(funcao_teste_servico)"
      ],
      "metadata": {
        "id": "wD5vux5TJW41"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testar pipeline\n",
        "# pipeline.testar_pipeline()"
      ],
      "metadata": {
        "id": "5jjg0NSsJZIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Envia o código do modelo para o SINAPSES\n",
        "pipeline.enviar_treinado()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9bXZNDLJcVm",
        "outputId": "e7d16322-3413-4b3b-d7ab-daa737b79f98"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8455 - loss: 0.6176 - val_accuracy: 0.9125 - val_loss: 0.4040\n",
            "Epoch 2/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8991 - loss: 0.3513 - val_accuracy: 0.9125 - val_loss: 0.2167\n",
            "Epoch 3/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9043 - loss: 0.2182 - val_accuracy: 0.9125 - val_loss: 0.1707\n",
            "Epoch 4/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9076 - loss: 0.1897 - val_accuracy: 0.9438 - val_loss: 0.1382\n",
            "Epoch 5/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9503 - loss: 0.1183 - val_accuracy: 0.9563 - val_loss: 0.1194\n",
            "Epoch 6/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9737 - loss: 0.0950 - val_accuracy: 0.9563 - val_loss: 0.1067\n",
            "Epoch 7/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9837 - loss: 0.0676 - val_accuracy: 0.9563 - val_loss: 0.1026\n",
            "Epoch 8/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9839 - loss: 0.0681 - val_accuracy: 0.9625 - val_loss: 0.1008\n",
            "Epoch 9/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9842 - loss: 0.0539 - val_accuracy: 0.9688 - val_loss: 0.0969\n",
            "Epoch 10/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9917 - loss: 0.0358 - val_accuracy: 0.9688 - val_loss: 0.1039\n",
            "Epoch 11/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9891 - loss: 0.0566 - val_accuracy: 0.9688 - val_loss: 0.0972\n",
            "Epoch 12/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9837 - loss: 0.0427 - val_accuracy: 0.9688 - val_loss: 0.0978\n",
            "Epoch 13/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9945 - loss: 0.0266 - val_accuracy: 0.9688 - val_loss: 0.1020\n",
            "Epoch 14/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9927 - loss: 0.0472 - val_accuracy: 0.9688 - val_loss: 0.1007\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"
          ]
        }
      ]
    }
  ]
}